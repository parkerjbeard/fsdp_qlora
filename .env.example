# FSDP QLoRA Environment Variables Example

# Hugging Face Configuration
# HF_TOKEN=your_hugging_face_token_here

# Backend Configuration
# FSDP_BACKEND=auto  # Options: cuda, mps, mlx, cpu, auto

# Distributed Training
# WORLD_SIZE=-1  # Number of GPUs to use (-1 = all available)
# MASTER_ADDR=localhost
# MASTER_PORT=29500

# Memory Configuration
# PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.7  # For MPS backend
# PYTORCH_MPS_LOW_WATERMARK_RATIO=0.5   # For MPS backend

# CPU Threading (for CPU offloading)
# OMP_NUM_THREADS=8      # Adjust based on CPU cores
# MKL_NUM_THREADS=8      # For Intel MKL

# Logging Configuration
# LOG_LEVEL=INFO         # Options: DEBUG, INFO, WARNING, ERROR
# LOG_FILE=training.log  # Optional log file path

# Model Cache
# HF_HOME=/path/to/model/cache
# TRANSFORMERS_CACHE=/path/to/transformers/cache

# Experiment Tracking
# WANDB_API_KEY=your_wandb_key_here
# WANDB_PROJECT=fsdp-qlora
# WANDB_MODE=offline     # For offline logging

# Performance Profiling
# TORCH_PROFILER_ENABLED=0
# PROFILE_OUTPUT_DIR=./profiling_results